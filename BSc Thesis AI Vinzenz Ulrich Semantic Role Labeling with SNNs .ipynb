{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic role labeling with spiking neural network using surrogate gradient learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With code from Zenke's notebook and https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/ and https://discuss.pytorch.org/t/rnn-to-predict-label-from-sequence-in-pytorch/154682\n",
    "For normalization: https://stackoverflow.com/questions/18380419/normalization-to-bring-in-the-range-of-0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, the input file and potentially already existing data to work with reside in the same directory on disk as this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GToZZA3A_GW0"
   },
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1687320781228
    },
    "id": "xzZefJ_C-w8D"
   },
   "outputs": [],
   "source": [
    "#importing\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.preprocessing as prepro\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix, cohen_kappa_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "\n",
    "import collections\n",
    "\n",
    "import io\n",
    "\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "\n",
    "np.random.seed(455)\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if code is run on a device where saving to disk is not possible or it is easier to print certain variables. Example: Microsoft Azure\n",
    "#torch.set_printoptions(threshold=sys.maxsize)\n",
    "#np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1687320784936
    },
    "id": "3hvWq69NI2uX"
   },
   "outputs": [],
   "source": [
    "#SURROGATE GRADIENT FUNCTION TO BE USED IN BACKPROPAGATION THROUGH TIME\n",
    "\n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements\n",
    "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid\n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which\n",
    "        we need to later backpropagate our error signals. To achieve this we use the\n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the\n",
    "        surrogate gradient of the loss with respect to the input.\n",
    "        Here we use the normalized negative part of a fast sigmoid\n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "# overwriting spike function with the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient function\n",
    "spike_fn  = SurrGradSpike.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1687320785290
    },
    "id": "Mfi6GUHtX8oo"
   },
   "outputs": [],
   "source": [
    "#FUNCTION TO RUN THE NETWORK\n",
    "\n",
    "def run_snn(inputs, nb_hidden, nb_outputs, w1, w2, w3=None, recurrent_on=False):\n",
    "    num_sequences_input = inputs.shape[0]\n",
    "\n",
    "    h1 = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
    "    syn = torch.zeros((num_sequences_input, nb_hidden), device=device, dtype=dtype)\n",
    "    mem = torch.zeros((num_sequences_input, nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    temp_recurrent = torch.zeros((num_sequences_input, nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "\n",
    "    # Compute hidden layer activity\n",
    "    for t in range(inputs.shape[1]):\n",
    "        mthr = mem - 1.0\n",
    "        out = SurrGradSpike.apply(mthr)\n",
    "        rst = out.detach() # To not propagate the error through the reset term\n",
    "\n",
    "        new_syn = 1\n",
    "        if recurrent_on:\n",
    "            new_syn = alpha * syn + (delta * temp_recurrent + (1 - delta) * h1[:, t])\n",
    "        else:\n",
    "            new_syn = alpha * syn + h1[:, t]\n",
    "\n",
    "        new_mem = (beta * mem + syn) * (1.0 - rst)\n",
    "\n",
    "        mem_rec.append(mem)\n",
    "        spk_rec.append(out)\n",
    "\n",
    "        mem = new_mem\n",
    "        syn = new_syn\n",
    "        if recurrent_on:\n",
    "            temp_recurrent = (w3 @ out.transpose(0, 1)).transpose(0, 1)\n",
    "\n",
    "    mem_rec = torch.stack(mem_rec, dim=1)\n",
    "    spk_rec = torch.stack(spk_rec, dim=1)\n",
    "\n",
    "    # Readout layer\n",
    "    h2 = torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
    "    flt = torch.zeros((num_sequences_input, nb_outputs), device=device, dtype=dtype)\n",
    "    out = torch.zeros((num_sequences_input, nb_outputs), device=device, dtype=dtype)\n",
    "    out_rec = []\n",
    "    for t in range(inputs.shape[1]):\n",
    "        new_flt = alpha * flt + h2[:, t]\n",
    "        new_out = beta * out + flt\n",
    "\n",
    "        flt = new_flt\n",
    "        out = new_out\n",
    "\n",
    "        out_rec.append(out)\n",
    "\n",
    "    out_rec = torch.stack(out_rec, dim=1)\n",
    "    other_recs = [mem_rec, spk_rec]\n",
    "    return out_rec, other_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1687321453557
    },
    "id": "IB3TzG1F1moK",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def process_data(header, until_this=0, data_string=None, exact_length=0, limit_length=0, keep_labels=0):\n",
    "    # READING THE DATA\n",
    "\n",
    "    # CHANGE FILE LOCATION TO WHERE THE INPUT FILE RESIDES ON DISK\n",
    "    f = open(r'./sentenceInput.txt', 'r')\n",
    "\n",
    "    lineCounter = 0\n",
    "    header1 = ''\n",
    "    text = ''\n",
    "    data = []\n",
    "    # from https://www.geeksforgeeks.org/read-a-file-line-by-line-in-python/\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line != '':\n",
    "            if lineCounter == 0:\n",
    "                header1 = line\n",
    "            else:\n",
    "                # from https://note.nkmk.me/en/python-split-rsplit-splitlines-re/\n",
    "                data.append(line.rstrip('\\n'))\n",
    "                data.append(f.readline().rstrip('\\n').split())\n",
    "                data.append([re.sub(\"'\", \"\", x) for x in f.readline().strip('\\n[]').split(\", \")])\n",
    "                f.readline()\n",
    "            lineCounter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    grouped_data = [data[n:n + 3] for n in range(0, len(data), 3)]\n",
    "\n",
    "    # LIMIT SENTENCE LENGTH TO SPECIFIED LENGTH AND FILTER OUT LESS COMMON LABELS, TO REDUCE COMPLEXITY\n",
    "\n",
    "    # if exact_length is 0, append sequences of all lengths below limit.\n",
    "    # if exact_length is a non-zero number, only append sequences of that length\n",
    "    exact_length = exact_length\n",
    "    limit_length = limit_length\n",
    "\n",
    "    temp = []\n",
    "    if exact_length > 0:\n",
    "        limit_length = exact_length\n",
    "    if limit_length > 0:\n",
    "        for chunk in grouped_data:\n",
    "            if len(chunk[1]) < (limit_length + 1):\n",
    "                if exact_length == limit_length:\n",
    "                    if len(chunk[1]) > (limit_length - 1):\n",
    "                        temp.append(chunk)\n",
    "                else:\n",
    "                    temp.append(chunk)\n",
    "    else:\n",
    "        temp = grouped_data\n",
    "\n",
    "    keep_labels = keep_labels\n",
    "    grouped_data_filtered = []\n",
    "    if keep_labels > 0:\n",
    "        counterObject = collections.Counter([text for chunk in temp for text in chunk[2]])\n",
    "        difference = set([text for chunk in temp for text in chunk[2]]) - set(\n",
    "            [twople[0] for twople in counterObject.most_common(keep_labels)])\n",
    "        for chunk in temp:\n",
    "            if not difference.intersection(set(chunk[2])):\n",
    "                grouped_data_filtered.append(chunk)\n",
    "\n",
    "        print(\"Five most common roles:\", set([twople[0] for twople in counterObject.most_common(5)]))\n",
    "        print(\"To be removed:\", difference)\n",
    "    else:\n",
    "        grouped_data_filtered = temp\n",
    "\n",
    "    print(\"All roles:\", set([text for chunk in grouped_data for text in chunk[2]]))\n",
    "    print(\"Roles that are still present in sentences after filtering out less common ones:\",\n",
    "          set([text for chunk in grouped_data_filtered for text in chunk[2]]))\n",
    "\n",
    "    # SPLIT DATA INTO INPUT, OUTPUT, AND IDENTIFIERS. ALSO REMOVE DASHES\n",
    "    identifiers = [chunk[0] for chunk in grouped_data_filtered]\n",
    "    input_data_old = [chunk[1] for chunk in grouped_data_filtered]\n",
    "    input_data = []\n",
    "    for sentence in input_data_old:\n",
    "        temp_sentence = []\n",
    "        for word in sentence:\n",
    "            if \"-\" in word:\n",
    "                temp_sentence.append(word[1:])\n",
    "            else:\n",
    "                temp_sentence.append(word)\n",
    "        input_data.append(temp_sentence)\n",
    "    output_data = [chunk[2] for chunk in grouped_data_filtered]\n",
    "\n",
    "    # POSSIBLE INPUTS AND POSSIBLE LABELS, MAPPING WORDS TO NUMBERS FOR ONE-HOT ENCODING\n",
    "\n",
    "    # Join all the sentences together and extract the unique words from the combined sentences\n",
    "    words_input_extracted = set([text for sublist in input_data for text in sublist])\n",
    "    words_input = header['fullwordlist']\n",
    "    words_output = set([text for sublist in output_data for text in sublist])\n",
    "\n",
    "    # check if all sequences have the same length, otherwise add padding to output word list\n",
    "    add_padding = False\n",
    "    only_this_length = len(output_data[1])\n",
    "    index = 0\n",
    "    while not add_padding and index < len(output_data):\n",
    "        if len(output_data[index]) != only_this_length:\n",
    "            add_padding = True\n",
    "        index += 1\n",
    "    if add_padding:\n",
    "        words_output = {'PADDING'}.union(words_output)\n",
    "\n",
    "    words = set(list(words_input) + list(words_output))\n",
    "\n",
    "    # Creating a dictionary that maps integers to the words\n",
    "    int2word = dict(enumerate(words))\n",
    "\n",
    "    # Creating another dictionary that maps words to integers\n",
    "    word2int = {word: ind for ind, word in int2word.items()}\n",
    "\n",
    "    # Creating a dictionary that maps integers to the words\n",
    "    int2word_output = dict(enumerate(words_output))\n",
    "\n",
    "    # Creating another dictionary that maps words to integers\n",
    "    word2int_output = {word: ind for ind, word in int2word_output.items()}\n",
    "\n",
    "    # THE MAXIMUM LENGTH IN TERMS OF WORDS IN A SENTENCE\n",
    "\n",
    "    maxlen = 0\n",
    "    maxindex = 0\n",
    "    for i in range(len(input_data)):\n",
    "        if len(input_data[i]) > maxlen:\n",
    "            maxlen = len(input_data[i])\n",
    "            maxindex = i\n",
    "\n",
    "    # maximum length for input and output are necessarily the same.\n",
    "\n",
    "    # THE MAXIMUM LENGTH IN TERMS OF CHARACTERS IN A SENTENCE\n",
    "\n",
    "    maxlen_char = 0\n",
    "    maxindex_char = (0, 0)\n",
    "    for i in range(len(input_data)):\n",
    "        # for j in range(len(input_data[i])):\n",
    "        temp = \"\"\n",
    "        for j in range(len(input_data[i])):\n",
    "            if not \"-\" in input_data[i][j]:\n",
    "                temp = temp + (input_data[i][j])\n",
    "            else:\n",
    "                temp = temp + (input_data[i][j][1:])\n",
    "        if len(temp) > maxlen_char:\n",
    "            maxlen_char = len(temp)\n",
    "            maxindex_char = i\n",
    "\n",
    "    # maximum length for input and output are necessarily the same.\n",
    "\n",
    "    full_input = copy.deepcopy(input_data)\n",
    "    full_output = copy.deepcopy(output_data)\n",
    "\n",
    "    until_this = until_this\n",
    "    input_data = full_input[:until_this]\n",
    "    output_data = full_output[:until_this]\n",
    "\n",
    "    return grouped_data, grouped_data_filtered, identifiers, input_data_old, full_input, input_data, full_output, output_data, words_input_extracted, words_input, words_output, words, int2word, word2int, int2word_output, word2int_output, maxlen, maxindex, maxlen_char, maxindex_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1687320785894
    },
    "id": "tNDNyjq21moK",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "\n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_dash(words_input, word_wo_dash):\n",
    "    #Helper function to make comparison between original input and input stripped of \"-\" possible\n",
    "    for word in words_input:\n",
    "        if word.replace('-', '') == word_wo_dash:\n",
    "            return word\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1687321701135
    },
    "id": "f1oqzQh51moL",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def make_x_y(input_data, output_data, nb_inputs, nb_hidden, nb_outputs, time_step, nb_steps, freq, words_output,\n",
    "             words_input, maxlen, maxlen_char, input_data_old, word2int_output, until_this, neurons_per_word=0,\n",
    "             nb_steps_char=0, spikes_words=None):\n",
    "    if neurons_per_word > 0:  # if you to a spatio-temporal encoding of the input\n",
    "        nb_inputs = neurons_per_word * len(words_input)\n",
    "    else:\n",
    "        nb_inputs = nb_inputs\n",
    "\n",
    "    spikes_words = spikes_words\n",
    "\n",
    "    if spikes_words is None:\n",
    "        spikes_words = []\n",
    "\n",
    "        if nb_steps_char > 0:\n",
    "            # CREATING SPIKE TRAINS FOR WORDS THAT ARE VARIABLE LENGTH DEPENDING ON THE INDIVIDUAL WORD LENGTH\n",
    "            for i in range(len(words_input)):\n",
    "                if neurons_per_word > 0:\n",
    "                    if \"-\" in words_input[i]:\n",
    "                        spikes_words.append(np.zeros((nb_inputs, len(words_input[i][1:]) * nb_steps_char)))\n",
    "                    else:\n",
    "                        spikes_words.append(np.zeros((nb_inputs, len(words_input[i]) * nb_steps_char)))\n",
    "                else:\n",
    "                    if \"-\" in words_input[i]:\n",
    "                        spikes_words.append(np.zeros((nb_inputs, len(words_input[i][1:]) * nb_steps_char)))\n",
    "                    else:\n",
    "                        spikes_words.append(np.zeros((nb_inputs, len(words_input[i]) * nb_steps_char)))\n",
    "\n",
    "            for i, word in enumerate(words_input):\n",
    "                # Generate spike pattern for each individual input word\n",
    "                temp = 1\n",
    "                if neurons_per_word > 0:\n",
    "                    temp2 = 1\n",
    "                    if \"-\" in words_input[i]:\n",
    "                        temp2 = np.random.poisson(freq * time_step, size=(\n",
    "                            neurons_per_word, len(words_input[i][1:]) * nb_steps_char))\n",
    "                        temp = np.zeros((nb_inputs, len(words_input[i][1:]) * nb_steps_char))\n",
    "                    else:\n",
    "                        temp2 = np.random.poisson(freq * time_step, size=(\n",
    "                            neurons_per_word, len(words_input[i]) * nb_steps_char))\n",
    "                        temp = np.zeros((nb_inputs, len(words_input[i]) * nb_steps_char))\n",
    "                    temp[words_input.index(word) * neurons_per_word: (words_input.index(word) + 1) * neurons_per_word,\n",
    "                    :] = temp2\n",
    "                else:\n",
    "                    if \"-\" in words_input[i]:\n",
    "                        temp = np.random.poisson(freq * time_step, size=(\n",
    "                            nb_inputs, len(words_input[i][1:]) * nb_steps_char))\n",
    "                    else:\n",
    "                        temp = np.random.poisson(freq * time_step, size=(\n",
    "                            nb_inputs, len(words_input[i]) * nb_steps_char))\n",
    "                temp = np.clip(temp, 0, 1)\n",
    "                # Assign spike pattern for each word to numpy array\n",
    "                spikes_words[i][:][:] = temp.tolist()\n",
    "        else:\n",
    "            # CREATING SPIKE TRAINS FOR WORDS THAT ARE OF FIXED LENGTH FOR EVERY WORD\n",
    "            spikes_words = 1\n",
    "            if neurons_per_word > 0:\n",
    "                spikes_words = np.random.poisson(freq * time_step, size=(len(words_input), neurons_per_word, nb_steps))\n",
    "                spikes_words = np.clip(spikes_words, 0, 1)\n",
    "                spikes_words = torch.tensor(spikes_words.transpose((2, 0, 1)).reshape(nb_steps, -1))\n",
    "            else:\n",
    "                spikes_words = np.random.poisson(freq * time_step, size=(len(words_input), nb_inputs, nb_steps))\n",
    "                spikes_words = np.clip(spikes_words, 0, 1)\n",
    "\n",
    "    # SENTENCE SPIKE TRAINS\n",
    "\n",
    "    sentence_spike_patterns = 1\n",
    "\n",
    "    if nb_steps_char > 0:\n",
    "        # CREATING SPIKE TRAINS FOR SENTENCES WITH WORDS THAT ARE OF VARIABLE LENGTH\n",
    "        # ALL SENTENCE SPIKE TRAINS ARE PADDED TO THE SAME LENGTH\n",
    "        for i, sentence in enumerate(input_data):\n",
    "            if i % 50 == 49:\n",
    "                print(\"processing...\", str(i + 1))\n",
    "                \n",
    "            sentence_spike_pattern = 1\n",
    "\n",
    "            for j, word in enumerate(sentence):\n",
    "                spike_word = np.transpose(spikes_words[words_input.index(put_dash(words_input, word))][:])\n",
    "\n",
    "                if j == 0:\n",
    "                    sentence_spike_pattern = spike_word\n",
    "                else:\n",
    "                    sentence_spike_pattern = np.concatenate((sentence_spike_pattern, spike_word), axis=0)\n",
    "                # sentence_spike_pattern.extend(spike_word)\n",
    "                # sentence_spike_pattern[j * nb_steps : (j + 1) * nb_steps, words_input.index(word) : (words_input.index(word)+1)] = spike_word\n",
    "\n",
    "            if not sentence_spike_pattern.shape[0] == (nb_steps_char * maxlen_char):\n",
    "                sentence_spike_pattern = np.concatenate((sentence_spike_pattern, np.zeros(\n",
    "                    (((nb_steps_char * maxlen_char) - sentence_spike_pattern.shape[0]), nb_inputs))), axis=0)\n",
    "\n",
    "            sentence_spike_pattern = torch.tensor(sentence_spike_pattern,\n",
    "                                                  dtype=dtype)\n",
    "\n",
    "            if i == 0:\n",
    "                sentence_spike_patterns = torch.reshape(sentence_spike_pattern, (\n",
    "                    1, sentence_spike_pattern.shape[0], sentence_spike_pattern.shape[1]))\n",
    "            else:\n",
    "                sentence_spike_patterns = torch.cat((sentence_spike_patterns, sentence_spike_pattern.unsqueeze(0)),\n",
    "                                                    dim=0)\n",
    "\n",
    "        print(\"sentence_spike_patterns.shape\", sentence_spike_patterns.shape)\n",
    "    else:\n",
    "        # CREATING SPIKE TRAINS FOR SENTENCES WHERE WORDS ARE OF FIXED LENGTH.\n",
    "        # ALL SENTENCE SPIKE TRAINS ARE PADDED TO THE SAME LENGTH\n",
    "\n",
    "        for i, sentence in enumerate(input_data_old[:until_this]):\n",
    "            if i % 50 == 49:\n",
    "                print(\"processing...\", str(i + 1))\n",
    "\n",
    "            sentence_spike_pattern = np.zeros((nb_steps * maxlen, nb_inputs))\n",
    "\n",
    "            for j, word in enumerate(sentence):\n",
    "                if neurons_per_word > 0:\n",
    "                    spike_word = spikes_words[:, words_input.index(word) * neurons_per_word: (words_input.index(\n",
    "                        word) + 1) * neurons_per_word]\n",
    "                    sentence_spike_pattern[j * nb_steps: (j + 1) * nb_steps,\n",
    "                    words_input.index(word) * neurons_per_word: (words_input.index(\n",
    "                        word) + 1) * neurons_per_word] = spike_word\n",
    "                else:\n",
    "                    spike_word = spikes_words[words_input.index(word), :, :]\n",
    "                    sentence_spike_pattern[j * nb_steps: (j + 1) * nb_steps, :] = np.transpose(spike_word)\n",
    "\n",
    "            sentence_spike_pattern = torch.tensor(sentence_spike_pattern, dtype=dtype)\n",
    "            if i == 0:\n",
    "                sentence_spike_patterns = torch.reshape(sentence_spike_pattern, (\n",
    "                    1, sentence_spike_pattern.shape[0], sentence_spike_pattern.shape[1]))\n",
    "            else:\n",
    "                sentence_spike_patterns = torch.cat((sentence_spike_patterns, sentence_spike_pattern.unsqueeze(0)),\n",
    "                                                    dim=0)\n",
    "        print(\"sentence_spike_patterns.shape\", sentence_spike_patterns.shape)\n",
    "\n",
    "    # CONVERTING OUTPUT LABELS TO NUMBERS CORRESPONDING TO THE RESPECTIVE LABELS\n",
    "\n",
    "    output_int = []\n",
    "    for i in range(len(output_data)):\n",
    "        temp = [word2int_output[word] for word in output_data[i]]\n",
    "        if 'PADDING' in words_output:  # add_padding:\n",
    "            output_int.append(temp + [word2int_output['PADDING']] * (maxlen - len(temp)))\n",
    "        else:\n",
    "            output_int.append(temp)\n",
    "\n",
    "    # MAKE THE OUTPUT LABELS INTO ONE-HOT ENCODED DATA\n",
    "\n",
    "    # TODO: from RNN_walkthrough\n",
    "\n",
    "    dict_size = len(word2int_output)\n",
    "    seq_len = maxlen\n",
    "\n",
    "    output_seq = one_hot_encode(output_int, dict_size, seq_len, len(output_int))\n",
    "    print(\"Output shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(output_seq.shape))\n",
    "\n",
    "    x_data = sentence_spike_patterns\n",
    "    y_data = torch.tensor(output_seq, dtype=dtype)\n",
    "\n",
    "    return x_data, y_data, spikes_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1687320786492
    },
    "id": "lwaAY5oX1moL",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def make_weights(beta, nb_inputs, nb_hidden, nb_outputs):\n",
    "    weight_scale = 7 * (1.0 - beta)\n",
    "\n",
    "    w1 = torch.empty((nb_inputs, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(w1, mean=0.0, std=weight_scale / np.sqrt(nb_inputs))\n",
    "\n",
    "    w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(w2, mean=0.0, std=weight_scale / np.sqrt(nb_hidden))\n",
    "\n",
    "    w3 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(w3, mean=0.0, std=weight_scale / np.sqrt(nb_hidden))\n",
    "    return w1, w2, w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1687321986391
    },
    "id": "vSJ3J6461moL",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def optimization(w1, w2, w3, input_data, x_data, y_data, epochs, learning_rate=2e-3, recurrent_on=False):\n",
    "    params = [w1, w2, w3]  # The parameters we want to optimize\n",
    "    optimizer = torch.optim.Adam(params, lr=2e-3, betas=(0.9, 0.999))  # The optimizer we are going to use\n",
    "\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)  # The log softmax function across output units\n",
    "    loss_fn = nn.CrossEntropyLoss()  # The cross entropy loss function\n",
    "\n",
    "    # The optimization loop\n",
    "    loss_hist = []\n",
    "    for e in range(epochs):\n",
    "        loss_over_epoch = []\n",
    "        print(\"new epoch number\", e)\n",
    "        for i, sequence in enumerate(x_data):\n",
    "            optimizer.zero_grad()\n",
    "            sequence = torch.unsqueeze(sequence, dim=0)\n",
    "            output, _ = run_snn(sequence, nb_hidden, nb_outputs, w1, w2, w3, recurrent_on)\n",
    "            chunks = []\n",
    "            for word in input_data[i]:\n",
    "                chunks.append(len(word) * nb_steps_char)\n",
    "            if sum(chunks) < x_data.shape[1]:\n",
    "                chunks.append((x_data.shape[1] - sum(chunks)))\n",
    "            seq_split = torch.split(torch.squeeze(output), split_size_or_sections=chunks, dim=0)\n",
    "\n",
    "            for j, word in enumerate(seq_split):\n",
    "                if j < y_data.shape[1]:\n",
    "                    log_p_y = log_softmax_fn(word[(word.shape[0] - 1), :][None, :]).squeeze()\n",
    "                    y = y_data[i][j]\n",
    "                    loss_val = loss_fn(log_p_y, y)\n",
    "                    loss_val.backward(retain_graph=True)\n",
    "                    loss_over_epoch.append(loss_val.item())\n",
    "\n",
    "            optimizer.step()\n",
    "        loss_hist.append(np.mean(loss_over_epoch))\n",
    "        print(\"Epoch:\", str(e) + \",\", \"Loss:\", loss_hist[-1])\n",
    "\n",
    "    return w1, w2, w3, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1687321068015
    },
    "id": "1zmf4yyzq-kU"
   },
   "outputs": [],
   "source": [
    "def voltage_pred(output, input_data, maxlen, words_output, x_data, y_data, word2int_output):\n",
    "    # turning the voltage traces into output labels\n",
    "    if type(output) is tuple:\n",
    "        output, _ = output\n",
    "        output = torch.squeeze(output)\n",
    "\n",
    "    predictions = torch.zeros(len(output), maxlen, len(words_output), dtype=torch.int64)\n",
    "    for sequence_index, sequence_data in enumerate(output):\n",
    "        chunks = []\n",
    "        for word in input_data[sequence_index]:\n",
    "            chunks.append(len(word) * nb_steps_char)\n",
    "        if sum(chunks) < x_data.shape[1]:\n",
    "            chunks.append((x_data.shape[1] - sum(chunks)))\n",
    "        sequence_data_splitted = torch.split(sequence_data, split_size_or_sections=chunks, dim=0)\n",
    "        for word_index, word in enumerate(sequence_data_splitted):\n",
    "            if word_index < y_data.shape[1]:\n",
    "                predictions[sequence_index, word_index] = word[(word.shape[0] - 1), :].argmax()\n",
    "    dict_size = len(word2int_output)\n",
    "    seq_len = maxlen\n",
    "    predictions = one_hot_encode(predictions, dict_size, seq_len, len(output))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1687320787347
    },
    "id": "HB8D3zBusNYP"
   },
   "outputs": [],
   "source": [
    "def accuracy_f1(predictions, y_data):\n",
    "    # calculating the accuracy and f1 score using the output predictions and the actual labels\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(y_data.shape[0]):\n",
    "        act_label = np.argmax(y_data[i])\n",
    "        pred_label = np.argmax(predictions[i])\n",
    "        if (act_label == pred_label):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = (correct / total)\n",
    "    f1 = f1_score(np.reshape(predictions, (predictions.shape[0] * predictions.shape[1], predictions.shape[2])),\n",
    "                  np.reshape(y_data, (y_data.shape[0] * y_data.shape[1], y_data.shape[2])), average='weighted')\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_voltage_traces(mem, spk=None, dim=(3,5), spike_height=5):\n",
    "    gs=GridSpec(*dim)\n",
    "    if spk is not None:\n",
    "        dat = 1.0*mem\n",
    "        dat[spk>0.0] = spike_height\n",
    "        dat = dat.detach().cpu().numpy()\n",
    "    else:\n",
    "        dat = mem.detach().cpu().numpy()\n",
    "    for i in range(np.prod(dim)):\n",
    "        if i==0: a0=ax=plt.subplot(gs[i])\n",
    "        else: ax=plt.subplot(gs[i],sharey=a0)\n",
    "        ax.plot(dat[i])\n",
    "        ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(x_complete, y_complete, input_complete, output_complete, train_indices, test_indices):\n",
    "    x_data = x_complete[train_indices]\n",
    "    y_data = y_complete[train_indices]\n",
    "    temp = []\n",
    "    for index in train_indices:\n",
    "        temp.append(input_complete[index])\n",
    "    input_data = temp\n",
    "    temp = []\n",
    "    for index in train_indices:\n",
    "        temp.append(output_complete[index])\n",
    "    output_data = temp\n",
    "\n",
    "    x_test = x_complete[test_indices]\n",
    "    y_test = y_complete[test_indices]\n",
    "    temp = []\n",
    "    for index in test_indices:\n",
    "        temp.append(input_complete[index])\n",
    "    input_test = temp\n",
    "    temp = []\n",
    "    for index in test_indices:\n",
    "        temp.append(output_complete[index])\n",
    "    output_test = temp\n",
    "\n",
    "    return x_data, y_data, input_data, output_data, x_test, y_test, input_test, output_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {'roles': 18, 'categorylist': ['DET', 'NOUNA', 'NOUNI', 'VERBUNACC', 'VERBLOC', 'VERBUNERG', 'VERBAPT', 'VERBTET', 'VERBCM', 'VERBD', 'ADJ', 'AUX', 'PRO', 'TO', 'ON', 'PER', 'INF', 'BY', 'PAR', 'BEING'], 'messages': 14, 'messagelist': ['animate intransitive', 'inanimate intransitive', 'agent-patient transitive active', 'agent-patient transitive passive', 'theme-experiencer transitive active', 'theme-experiencer transitive passive', 'transfer dative active, prepositional', 'transfer dative passive, prepositional', 'transfer dative active, double object', 'transfer dative passive, double object', 'caused motion active', 'caused motion passive', 'locative active', 'PLOC'], 'fullwordlist': ['the', 'a', 'prn', 'man', 'woman', 'cat', 'dog', 'boy', 'girl', 'father', 'mother', 'ball', 'stick', 'apple', 'cake', 'orange', 'banana', 'phone', 'cup', 'sleep', 'jump', 'dance', 'sit', 'go', 'walk', 'drive', 'run', 'open', 'close', 'break', 'smash', 'kick', 'chase', 'touch', 'eat', 'scare', 'surprise', 'bother', 'hurt', 'put', 'hit', 'carry', 'push', 'give', 'throw', 'show', 'present', 'red', 'beautiful', 'nice', 'big', 'small', 'great', 'old', 'smelly', 'is', 'was', 'are', 'were', 'he', 'she', 'it', 'him', 'her', 'they', 'them', 'to', 'on', '.', '-ing', '-ss', '-ed', '-s', 'by', '-par', 'being'], 'words': 76, 'allroles': set(['AGENT_N0', 'AGENT_N1', 'AGENT_N2', 'EOS', 'ACTION_V0', 'RECIPIENT_N2', 'RECIPIENT_N0', 'RECIPIENT_N1', 'GOAL_N1', 'GOAL_N0', 'GOAL_N2', 'EXPERIENCER_N1', 'EXPERIENCER_N0', 'THEME_N0', 'THEME_N1', 'THEME_N2', 'PATIENT_N0', 'PATIENT_N1']), 'categories': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_experiment = [False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# until_this represents how many sequences will be generated at most in total.\n",
    "# IT NEEDS TO BE AT LEAST A TWO-DIGIT, NUMBER.\n",
    "until_this_here = 10\n",
    "epochs_all = 1\n",
    "train_perc = 0.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring variables so they can be used later\n",
    "neurons_per_word,nb_inputs,nb_hidden,nb_outputs,time_step,nb_steps,nb_steps_char,freq,x_data,y_data,input_data,output_data,maxlen,words_output,words_input,word2int_output,x_test,y_test,input_test,output_test = 1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_experiment = []\n",
    "weights_experiment = []\n",
    "acc_experiment = []\n",
    "acc_test_experiment = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal models (one feedforward, one recurrent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists(\"./data_temporal.pkl\"):\n",
    "    with open('./data_temporal.pkl', 'rb') as f:\n",
    "        nb_inputs, nb_hidden, nb_outputs, time_step, nb_steps, freq, words_input_extracted, words_input, words_output, word2int, int2word, int2word_output, word2int_output, maxlen, maxlen_char, complete_y, input_data_old, complete_input, complete_output, train_indices, test_indices, until_this_here, neurons_per_word, nb_steps_char, spikes_words = pickle.load(\n",
    "            f)\n",
    "    print(\"read done data_temporal\")\n",
    "    x_data, y_data, spikes_words = make_x_y(complete_input, complete_output, nb_inputs, nb_hidden, nb_outputs,\n",
    "                                            time_step, nb_steps, freq, words_output, words_input, maxlen,\n",
    "                                            maxlen_char,\n",
    "                                            input_data_old, word2int_output, until_this_here,\n",
    "                                            nb_steps_char=nb_steps_char, spikes_words=spikes_words)\n",
    "    complete_x = copy.deepcopy(x_data)\n",
    "    complete_y = copy.deepcopy(y_data)\n",
    "    x_data, y_data, input_data, output_data, x_test, y_test, input_test, output_test = split_train_test(complete_x,\n",
    "                                                                                                        complete_y,\n",
    "                                                                                                        complete_input,\n",
    "                                                                                                        complete_output,\n",
    "                                                                                                        train_indices,\n",
    "                                                                                                        test_indices)\n",
    "    print(\"num inputs:\", str(nb_inputs) + \",\", \"num hidden:\", str(nb_hidden) + \",\", \"num outputs:\", nb_outputs)\n",
    "    print(\"After train test split x_data.shape\", x_data.shape)\n",
    "    print(\"After train test split y_data.shape\", y_data.shape)\n",
    "    print(\"After train test split len(input_data)\", len(input_data))\n",
    "    print(\"After train test split len(output_data)\", len(output_data))\n",
    "\n",
    "else:\n",
    "    grouped_data, grouped_data_filtered, identifiers, input_data_old, full_input, input_data, full_output, output_data, words_input_extracted, words_input, words_output, words, int2word, word2int, int2word_output, word2int_output, maxlen, maxindex, maxlen_char, maxindex_char = process_data(\n",
    "        header, until_this_here, limit_length=10, keep_labels=5)\n",
    "    complete_input = copy.deepcopy(input_data)\n",
    "    complete_output = copy.deepcopy(output_data)\n",
    "    # print(\"words_input\", words_input)\n",
    "    # print(\"words_input_extracted\", words_input_extracted)\n",
    "    # print(\"len(words_input)\", len(words_input))\n",
    "    # print(\"len(words_input_extracted)\", len(words_input_extracted))\n",
    "\n",
    "    neurons_per_word = 0  # neurons_experiment[i]\n",
    "    nb_inputs = 100  # neurons_per_word * len(words_input)\n",
    "    nb_hidden = 25\n",
    "    nb_outputs = len(words_output)\n",
    "    print(\"num inputs:\", str(nb_inputs) + \",\", \"num hidden:\", str(nb_hidden) + \",\", \"num outputs:\", nb_outputs)\n",
    "    time_step = 1e-3\n",
    "    nb_steps = 200\n",
    "    nb_steps_char = 50\n",
    "    # batch_size = until_this_here\n",
    "    freq = 100  # Hz\n",
    "\n",
    "    x_data, y_data, spikes_words = make_x_y(complete_input, complete_output, nb_inputs, nb_hidden, nb_outputs,\n",
    "                                            time_step, nb_steps, freq, words_output, words_input, maxlen,\n",
    "                                            maxlen_char,\n",
    "                                            input_data_old, word2int_output, until_this_here,\n",
    "                                            nb_steps_char=nb_steps_char)\n",
    "\n",
    "    train_indices = random.sample(range(0, until_this_here), int(until_this_here * train_perc))\n",
    "    test_indices = [num for num in range(0, until_this_here)]\n",
    "    test_indices = [x for x in test_indices if x not in train_indices]\n",
    "\n",
    "    complete_x = copy.deepcopy(x_data)\n",
    "    complete_y = copy.deepcopy(y_data)\n",
    "\n",
    "    x_data, y_data, input_data, output_data, x_test, y_test, input_test, output_test = split_train_test(complete_x,\n",
    "                                                                                                        complete_y,\n",
    "                                                                                                        complete_input,\n",
    "                                                                                                        complete_output,\n",
    "                                                                                                        train_indices,\n",
    "                                                                                                        test_indices)\n",
    "\n",
    "    print(\"After train test split x_data.shape\", x_data.shape)\n",
    "    print(\"After train test split y_data.shape\", y_data.shape)\n",
    "    print(\"After train test split len(input_data)\", len(input_data))\n",
    "    print(\"After train test split len(output_data)\", len(output_data))\n",
    "\n",
    "    with open('./data_temporal.pkl', 'wb') as f:\n",
    "        pickle.dump(\n",
    "            (nb_inputs, nb_hidden, nb_outputs, time_step, nb_steps, freq, words_input_extracted,\n",
    "             words_input, words_output, word2int, int2word, int2word_output, word2int_output, maxlen,\n",
    "             maxlen_char, complete_y, input_data_old, complete_input, complete_output, train_indices,\n",
    "             test_indices, until_this_here, neurons_per_word, nb_steps_char, spikes_words), f)\n",
    "    print(\"write done data_temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words_input)\n",
    "print(words_input_extracted)\n",
    "print(len(words_input))\n",
    "print(len(words_input_extracted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))\n",
    "delta = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward temporal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nfeedforward temporal model\")\n",
    "w1_temp_1, w2_temp_1, w3_temp_1 = make_weights(beta, nb_inputs, nb_hidden, nb_outputs)\n",
    "\n",
    "w1_temp_1, w2_temp_1, w3_temp_1, loss_hist_temp_1 = optimization(w1_temp_1, w2_temp_1, w3_temp_1, input_data,\n",
    "                                                                 x_data, y_data, epochs=epochs_all,\n",
    "                                                                 learning_rate=5e-4,\n",
    "                                                                 recurrent_on=False)\n",
    "\n",
    "accuracy_temp_1, f1_temp_1 = accuracy_f1(\n",
    "    voltage_pred(run_snn(x_data, nb_hidden, nb_outputs, w1_temp_1, w2_temp_1, w3_temp_1, recurrent_on=False),\n",
    "                 input_data, maxlen, words_output, x_data, y_data, word2int_output), y_data)\n",
    "accuracy_test_temp_1, f1_test_temp_1 = accuracy_f1(\n",
    "    voltage_pred(run_snn(x_test, nb_hidden, nb_outputs, w1_temp_1, w2_temp_1, w3_temp_1, recurrent_on=False),\n",
    "                 input_test, maxlen, words_output, x_test, y_test, word2int_output), y_test)\n",
    "\n",
    "loss_experiment.append(loss_hist_temp_1)\n",
    "weights_experiment.append([w1_temp_1, w2_temp_1, w3_temp_1])\n",
    "acc_experiment.append([accuracy_temp_1, f1_temp_1])\n",
    "acc_test_experiment.append([accuracy_test_temp_1, f1_test_temp_1])\n",
    "\n",
    "with open('./results_after_temporal1.pkl', 'wb') as f:\n",
    "    pickle.dump((loss_experiment, weights_experiment, acc_experiment, acc_test_experiment), f)\n",
    "\n",
    "print(\"write done results_after_temporal1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent temporal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1687401522711
    },
    "id": "GYJ4_PBw1moW",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))\n",
    "delta = 0.2\n",
    "\n",
    "#recurrent temporal model\n",
    "\n",
    "print(\"\\nrecurrent temporal model\")\n",
    "w1_temp_2, w2_temp_2, w3_temp_2 = make_weights(beta, nb_inputs, nb_hidden, nb_outputs)\n",
    "\n",
    "w1_temp_2, w2_temp_2, w3_temp_2, loss_hist_temp_2 = optimization(w1_temp_2, w2_temp_2, w3_temp_2, input_data,\n",
    "                                                                 x_data, y_data, epochs=epochs_all,\n",
    "                                                                 learning_rate=5e-4,\n",
    "                                                                 recurrent_on=True)\n",
    "\n",
    "accuracy_temp_2, f1_temp_2 = accuracy_f1(\n",
    "    voltage_pred(run_snn(x_data, nb_hidden, nb_outputs, w1_temp_2, w2_temp_2, w3_temp_2, recurrent_on=True),\n",
    "                 input_data, maxlen, words_output, x_data, y_data, word2int_output), y_data)\n",
    "accuracy_test_temp_2, f1_test_temp_2 = accuracy_f1(\n",
    "    voltage_pred(run_snn(x_test, nb_hidden, nb_outputs, w1_temp_2, w2_temp_2, w3_temp_2, recurrent_on=True),\n",
    "                 input_test, maxlen, words_output, x_test, y_test, word2int_output), y_test)\n",
    "\n",
    "loss_experiment.append(loss_hist_temp_2)\n",
    "weights_experiment.append([w1_temp_2, w2_temp_2, w3_temp_2])\n",
    "acc_experiment.append([accuracy_temp_2, f1_temp_2])\n",
    "acc_test_experiment.append([accuracy_test_temp_2, f1_test_temp_2])\n",
    "\n",
    "with open('./results_after_temporal2.pkl', 'wb') as f:\n",
    "    pickle.dump((loss_experiment, weights_experiment, acc_experiment, acc_test_experiment), f)\n",
    "\n",
    "print(\"write done results_after_temporal2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatio-temporal models (one feedforward, one recurrent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if exists(\"./data_spatio_temporal.pkl\"):\n",
    "    with open('./data_spatio_temporal.pkl', 'rb') as f:\n",
    "        nb_inputs, nb_hidden, nb_outputs, time_step, nb_steps, freq, words_input_extracted, words_input, words_output, word2int, int2word, int2word_output, word2int_output, maxlen, maxlen_char, complete_y, input_data_old, complete_input, complete_output, train_indices, test_indices, until_this_here, neurons_per_word, nb_steps_char, spikes_words = pickle.load(f)\n",
    "    print(\"read done data_spatio_temporal\")\n",
    "    x_data, y_data, spikes_words = make_x_y(complete_input, complete_output, nb_inputs, nb_hidden, nb_outputs,\n",
    "                                            time_step, nb_steps, freq, words_output, words_input, maxlen,\n",
    "                                            maxlen_char,\n",
    "                                            input_data_old, word2int_output, until_this_here,\n",
    "                                            nb_steps_char=nb_steps_char, spikes_words=spikes_words)\n",
    "    complete_x = copy.deepcopy(x_data)\n",
    "    complete_y = copy.deepcopy(y_data)\n",
    "    x_data, y_data, input_data, output_data, x_test, y_test, input_test, output_test = split_train_test(complete_x,\n",
    "                                                                                                        complete_y,\n",
    "                                                                                                        complete_input,\n",
    "                                                                                                        complete_output,\n",
    "                                                                                                        train_indices,\n",
    "                                                                                                        test_indices)\n",
    "    print(\"num inputs:\", str(nb_inputs) + \",\", \"num hidden:\", str(nb_hidden) + \",\", \"num outputs:\", nb_outputs)\n",
    "    print(\"After train test split x_data.shape\", x_data.shape)\n",
    "    print(\"After train test split y_data.shape\", y_data.shape)\n",
    "    print(\"After train test split len(input_data)\", len(input_data))\n",
    "    print(\"After train test split len(output_data)\", len(output_data))\n",
    "\n",
    "else:\n",
    "    neurons_per_word = 5  # neurons_experiment[i]\n",
    "    nb_inputs = neurons_per_word * len(words_input)\n",
    "    nb_hidden = 25\n",
    "    nb_outputs = len(words_output)\n",
    "    print(\"num inputs:\", str(nb_inputs) + \",\", \"num hidden:\", str(nb_hidden) + \",\", \"num outputs:\", nb_outputs)\n",
    "    time_step = 1e-3\n",
    "    nb_steps = 200\n",
    "    nb_steps_char = 50\n",
    "    # batch_size = until_this_here\n",
    "    freq = 100  # Hz\n",
    "\n",
    "    x_data = 1\n",
    "    y_data = 1\n",
    "    print(x_data, y_data)\n",
    "\n",
    "    x_data, y_data, spikes_words = make_x_y(complete_input, complete_output, nb_inputs, nb_hidden, nb_outputs,\n",
    "                                            time_step, nb_steps, freq, words_output, words_input, maxlen,\n",
    "                                            maxlen_char,\n",
    "                                            input_data_old, word2int_output, until_this_here,\n",
    "                                            neurons_per_word=neurons_per_word, nb_steps_char=nb_steps_char)\n",
    "\n",
    "    train_indices = random.sample(range(0, until_this_here), int(until_this_here * train_perc))\n",
    "    test_indices = [num for num in range(0, until_this_here)]\n",
    "    test_indices = [x for x in test_indices if x not in train_indices]\n",
    "\n",
    "    complete_x = copy.deepcopy(x_data)\n",
    "    complete_y = copy.deepcopy(y_data)\n",
    "\n",
    "    x_data, y_data, input_data, output_data, x_test, y_test, input_test, output_test = split_train_test(complete_x,\n",
    "                                                                                                        complete_y,\n",
    "                                                                                                        complete_input,\n",
    "                                                                                                        complete_output,\n",
    "                                                                                                        train_indices,\n",
    "                                                                                                        test_indices)\n",
    "\n",
    "    print(\"After train test split x_data.shape\", x_data.shape)\n",
    "    print(\"After train test split y_data.shape\", y_data.shape)\n",
    "    print(\"After train test split len(input_data)\", len(input_data))\n",
    "    print(\"After train test split len(output_data)\", len(output_data))\n",
    "\n",
    "    with open('./data_spatio_temporal.pkl', 'wb') as f:\n",
    "        pickle.dump((nb_inputs, nb_hidden, nb_outputs, time_step, nb_steps, freq, words_input_extracted,\n",
    "                     words_input, words_output, word2int, int2word, int2word_output, word2int_output, maxlen,\n",
    "                     maxlen_char, complete_y, input_data_old, complete_input, complete_output, train_indices,\n",
    "                     test_indices, until_this_here, neurons_per_word, nb_steps_char, spikes_words), f)\n",
    "\n",
    "    print(\"write done data_spatio_temporal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward spatio-temporal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "\n",
    "alpha = float(np.exp(-time_step / tau_syn))\n",
    "beta = float(np.exp(-time_step / tau_mem))\n",
    "delta = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nfeedforward spatio-temporal model\")\n",
    "\n",
    "w1_spatemp_1, w2_spatemp_1, w3_spatemp_1 = make_weights(beta, nb_inputs, nb_hidden, nb_outputs)\n",
    "\n",
    "w1_spatemp_1, w2_spatemp_1, w3_spatemp_1, loss_hist_spatemp_1 = optimization(w1_spatemp_1, w2_spatemp_1,\n",
    "                                                                             w3_spatemp_1, input_data, x_data,\n",
    "                                                                             y_data, epochs=epochs_all,\n",
    "                                                                             learning_rate=5e-4,\n",
    "                                                                             recurrent_on=False)\n",
    "\n",
    "accuracy_spatemp_1, f1_spatemp_1 = accuracy_f1(voltage_pred(\n",
    "    run_snn(x_data, nb_hidden, nb_outputs, w1_spatemp_1, w2_spatemp_1, w3_spatemp_1, recurrent_on=False),\n",
    "    input_data, maxlen, words_output, x_data, y_data, word2int_output), y_data)\n",
    "accuracy_test_spatemp_1, f1_test_spatemp_1 = accuracy_f1(voltage_pred(\n",
    "    run_snn(x_test, nb_hidden, nb_outputs, w1_spatemp_1, w2_spatemp_1, w3_spatemp_1, recurrent_on=False),\n",
    "    input_test, maxlen, words_output, x_test, y_test, word2int_output), y_test)\n",
    "\n",
    "loss_experiment.append(loss_hist_spatemp_1)\n",
    "weights_experiment.append([w1_spatemp_1, w2_spatemp_1, w3_spatemp_1])\n",
    "acc_experiment.append([accuracy_spatemp_1, f1_spatemp_1])\n",
    "acc_test_experiment.append([accuracy_test_spatemp_1, f1_test_spatemp_1])\n",
    "\n",
    "with open('./results_after_spatio_temporal1.pkl', 'wb') as f:\n",
    "    pickle.dump((loss_experiment, weights_experiment, acc_experiment, acc_test_experiment), f)\n",
    "\n",
    "print(\"write done results_after_spatio_temporal1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent spatio-temporal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "\n",
    "alpha = float(np.exp(-time_step / tau_syn))\n",
    "beta = float(np.exp(-time_step / tau_mem))\n",
    "delta = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"\\nrecurrent spatio-temporal model\")\n",
    "\n",
    "w1_spatemp_2, w2_spatemp_2, w3_spatemp_2 = make_weights(beta, nb_inputs, nb_hidden, nb_outputs)\n",
    "\n",
    "w1_spatemp_2, w2_spatemp_2, w3_spatemp_2, loss_hist_spatemp_2 = optimization(w1_spatemp_2, w2_spatemp_2,\n",
    "                                                                             w3_spatemp_2, input_data, x_data,\n",
    "                                                                             y_data, epochs=epochs_all,\n",
    "                                                                             learning_rate=5e-4,\n",
    "                                                                             recurrent_on=True)\n",
    "\n",
    "accuracy_spatemp_2, f1_spatemp_2 = accuracy_f1(voltage_pred(\n",
    "    run_snn(x_data, nb_hidden, nb_outputs, w1_spatemp_2, w2_spatemp_2, w3_spatemp_2, recurrent_on=True), input_data,\n",
    "    maxlen, words_output, x_data, y_data, word2int_output), y_data)\n",
    "accuracy_test_spatemp_2, f1_test_spatemp_2 = accuracy_f1(voltage_pred(\n",
    "    run_snn(x_test, nb_hidden, nb_outputs, w1_spatemp_2, w2_spatemp_2, w3_spatemp_2, recurrent_on=True), input_test,\n",
    "    maxlen, words_output, x_test, y_test, word2int_output), y_test)\n",
    "\n",
    "loss_experiment.append(loss_hist_spatemp_2)\n",
    "weights_experiment.append([w1_spatemp_2, w2_spatemp_2, w3_spatemp_2])\n",
    "acc_experiment.append([accuracy_spatemp_2, f1_spatemp_2])\n",
    "acc_test_experiment.append([accuracy_test_spatemp_2, f1_test_spatemp_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results_after_spatio_temporal2.pkl', 'wb') as f:\n",
    "    pickle.dump((loss_experiment, weights_experiment, acc_experiment, acc_test_experiment), f)\n",
    "\n",
    "print(\"write done results_after_spatio_temporal2\")\n",
    "\n",
    "print(\"length loss_experiment:\", len(loss_experiment))\n",
    "print(\"length weights_experiment:\", len(weights_experiment))\n",
    "print(\"length acc_experiment:\", len(acc_experiment))\n",
    "print(\"length acc_test_experiment:\", len(acc_test_experiment))\n",
    "\n",
    "print(\"Training and test accuracies and F1 scores per model:\")\n",
    "for index, accuracy_and_f1 in enumerate(acc_experiment):\n",
    "    print(\"Model number \" + str(index + 1) + \" with training accuracy\", accuracy_and_f1[0], \"and training F1 score\",\n",
    "          accuracy_and_f1[1])\n",
    "for index, accuracy_and_f1 in enumerate(acc_test_experiment):\n",
    "    print(\"Model number \" + str(index + 1) + \" with test accuracy\", accuracy_and_f1[0], \"and test F1 score\",\n",
    "          accuracy_and_f1[1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
